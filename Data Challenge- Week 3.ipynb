{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1410336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! In order for this file to run without any import errors, it needs to be directly within the \"IML.HUJI\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9eab1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and general configurations\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "STUDENTS = ['313234940', '207906306', '204841423']\n",
    "\n",
    "# The file should be in the same folder as this notebook\n",
    "# WEEKLY_TEST_SET = './test_set_week_1.csv'\n",
    "# WEEKLY_TEST_SET = './test_set_week_2.csv'\n",
    "WEEKLY_TEST_SET = './test_set_week_3.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb2119f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data columns (by type)\n",
    "\n",
    "ID_COLS = ['h_booking_id', 'hotel_id', 'hotel_country_code', 'h_customer_id']\n",
    "\n",
    "DATETIME_COLS = ['booking_datetime', 'checkin_date', 'checkout_date', 'hotel_live_date']\n",
    "\n",
    "CODES_COLS = ['origin_country_code', 'hotel_area_code', 'hotel_city_code']\n",
    "\n",
    "# (dropped) 'hotel_brand_code', 'hotel_chain_code' (have ~43K nulls!)\n",
    "\n",
    "CATEGORICAL_COLS = ['accommadation_type_name', 'charge_option', 'customer_nationality',\n",
    "                    'guest_nationality_country_name', 'language', 'original_payment_method',\n",
    "                    'original_payment_type', 'original_payment_currency']\n",
    "\n",
    "NUMERICAL_COLS = ['hotel_star_rating', 'no_of_adults',  'no_of_children', 'no_of_extra_bed', 'no_of_room',\n",
    "                  'original_selling_amount']\n",
    "\n",
    "SHOULD_BE_BOOLEAN_COLS = ['guest_is_not_the_customer', 'request_nonesmoke', 'request_latecheckin',\n",
    "                          'request_highfloor', 'request_largebed', 'request_twinbeds', 'request_airport',\n",
    "                          'request_earlycheckin']\n",
    "\n",
    "# The following columns have 25040 nulls, we will treat null here as 0 (False):\n",
    "# request_nonesmoke, request_latecheckin, request_highfloor, request_largebed, request_twinbeds,\n",
    "# request_airport, request_earlycheckin\n",
    "# remove them - or fill in nan with 0\n",
    "\n",
    "BOOLEAN_COLS = ['is_user_logged_in', 'is_first_booking']\n",
    "\n",
    "LABEL_COL = 'cancellation_datetime'\n",
    "\n",
    "NO_SHOW_PATTERN = '_(\\d+)(N|P)'\n",
    "POLICY_PATTERN = '(\\d+)D(\\d+)(N|P)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d10a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data parsing functions\n",
    "\n",
    "def parse_all_cancellation_policy(data):\n",
    "    counter = 0\n",
    "    for i, row in data.iterrows():\n",
    "        #if i % 2000 == 0:\n",
    "        #    print(i)\n",
    "        \n",
    "        if row[\"cancellation_policy_code\"] == \"UNKNOWN\":\n",
    "            row[\"cancellation_policy_code\"] = '0D100P_100P'\n",
    "            counter += 1\n",
    "\n",
    "        policy = row[\"cancellation_policy_code\"]\n",
    "        n_nights = row[\"n_nights\"]\n",
    "\n",
    "        no_show = re.findall(NO_SHOW_PATTERN, policy)\n",
    "        cancel_policy = re.findall(POLICY_PATTERN, policy)\n",
    "        worse_policy = cancel_policy[-1]\n",
    "        basic_policy = cancel_policy[-2] if len(cancel_policy) > 1 else worse_policy\n",
    "        if no_show:\n",
    "            no_show_as_int = int(no_show[0][0])\n",
    "            if no_show[0][1] == \"N\":\n",
    "                no_show_days = no_show_as_int\n",
    "                no_show_percent = no_show_as_int / n_nights * 100\n",
    "            else:\n",
    "                no_show_days = no_show_as_int * n_nights / 100\n",
    "                no_show_percent = no_show_as_int\n",
    "\n",
    "        else:\n",
    "            worse_policy_without_no_show = re.findall(POLICY_PATTERN, policy)[-1]\n",
    "\n",
    "            if worse_policy_without_no_show[-1] == \"N\":\n",
    "                no_show_days = int(worse_policy_without_no_show[1])\n",
    "                no_show_percent = int(worse_policy_without_no_show[1]) / n_nights * 100\n",
    "            else:\n",
    "                no_show_days = int(worse_policy_without_no_show[1]) * n_nights / 100\n",
    "                no_show_percent = int(worse_policy_without_no_show[1])\n",
    "\n",
    "        \n",
    "        worse_policy_as_int = int(worse_policy[1])\n",
    "        basic_policy_as_int = int(basic_policy[1])\n",
    "        if worse_policy[2] == \"N\":\n",
    "            #nights = worse_policy_as_int\n",
    "            percent = worse_policy_as_int / n_nights * 100\n",
    "        else:\n",
    "            #nights = worse_policy_as_int * n_nights / 100\n",
    "            percent = worse_policy_as_int\n",
    "        if basic_policy[2] == \"N\":\n",
    "            #basic_by_nights = worse_policy_as_int\n",
    "            basic_percent = worse_policy_as_int / n_nights * 100\n",
    "        else:\n",
    "            #basic_by_nights = int(basic_policy_as_int) * n_nights / 100\n",
    "            basic_percent = int(basic_policy_as_int)\n",
    "        \n",
    "\n",
    "        data.loc[i, \"no_show_days\"] = no_show_days\n",
    "        data.loc[i, \"no_show_percentage\"] = no_show_percent\n",
    "\n",
    "        \n",
    "        days = int(worse_policy[0])\n",
    "        basic_days = int(basic_policy[0])\n",
    "        data.loc[i, \"basic_charge_percentage\"] = basic_percent\n",
    "        #data.loc[i, \"basic_charge_by_nights\"] = basic_by_nights\n",
    "        data.loc[i, \"basic_charge_days\"] = basic_days\n",
    "        #data.loc[i, \"basic_charge_days_times_nights\"] = basic_days * basic_by_nights\n",
    "        #data.loc[i, \"basic_charge_days_times_percentage\"] = basic_days * basic_percent\n",
    "        data.loc[i, \"charge_percentage\"] = percent\n",
    "        #data.loc[i, \"charge_by_nights\"] = nights\n",
    "        data.loc[i, \"charge_days\"] = days\n",
    "        #data.loc[i, \"charge_days_times_nights\"] = days * nights\n",
    "        #data.loc[i, \"charge_days_times_percentage\"] = days * percent\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_dates(data):\n",
    "    data = data.copy()\n",
    "    for col_name in DATETIME_COLS:\n",
    "        col_date_obj = data[col_name]\n",
    "        data[f\"{col_name}_year\"] = col_date_obj.dt.year\n",
    "        data[f\"{col_name}_month\"] = col_date_obj.dt.month\n",
    "        data[f\"{col_name}_day_in_week\"] = col_date_obj.dt.day_of_week\n",
    "        #data[f\"{col_name}_dayofyear\"] = col_date_obj.dt.dayofyear\n",
    "        data = pd.get_dummies(data, columns=[f\"{col_name}_year\", f\"{col_name}_month\", f\"{col_name}_day_in_week\"], drop_first=True)\n",
    "        \n",
    "    data[\"n_nights\"] = (data[\"checkout_date\"] - data[\"checkin_date\"]).dt.days\n",
    "    data[\"n_days_from_booking_to_checkin\"] = (data[\"checkin_date\"] - data[\"booking_datetime\"]).dt.days\n",
    "\n",
    "    data = data.drop(DATETIME_COLS, axis=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbe7c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(full_data, test=False, cols=None):\n",
    "    # choose some of the original columns\n",
    "    if test:\n",
    "        data = full_data[NUMERICAL_COLS + DATETIME_COLS + SHOULD_BE_BOOLEAN_COLS + BOOLEAN_COLS +\n",
    "                         CATEGORICAL_COLS + [\"cancellation_policy_code\"]]\n",
    "    else:\n",
    "        data = full_data[NUMERICAL_COLS + DATETIME_COLS + SHOULD_BE_BOOLEAN_COLS + BOOLEAN_COLS +\n",
    "                         CATEGORICAL_COLS + [\"cancellation_policy_code\"] + [LABEL_COL]]\n",
    "\n",
    "    data = data.copy()\n",
    "        \n",
    "    # edit labels\n",
    "    if not test:\n",
    "        #data[LABEL_COL] = data[LABEL_COL].notnull()  # todo maybe change to handle the dates?\n",
    "        booking_date_diff = (pd.to_datetime(data[LABEL_COL]) - data[\"booking_datetime\"]).dt.days.between(7, 44) \n",
    "        checkin_diff = (pd.to_datetime(data[LABEL_COL]) - data[\"checkin_date\"]).dt.days.between(2, 9) \n",
    "        \n",
    "        data[LABEL_COL] = (booking_date_diff) | (checkin_diff)\n",
    "        data = data.copy()\n",
    "\n",
    "    # fill in 0 instead of None's\n",
    "    for col_name in SHOULD_BE_BOOLEAN_COLS:\n",
    "        data.loc[:, col_name] = data.loc[:, col_name].fillna(0)\n",
    "\n",
    "    # handle datetime cols\n",
    "    data = parse_dates(data)\n",
    "    \n",
    "    # data = parse_all_cancellation_policy(data)\n",
    "    data = data.drop(['cancellation_policy_code'], axis=1)\n",
    "    \n",
    "    # replace categorical features with their dummies\n",
    "    data = pd.get_dummies(data, columns=CATEGORICAL_COLS, drop_first=True)\n",
    "    \n",
    "    data = data.copy()\n",
    "    if cols is not None:\n",
    "        missing_cols = set(cols) - set(data.columns)\n",
    "        for col in missing_cols:\n",
    "            data.loc[:, col] = 0\n",
    "        data = data.loc[:, cols] \n",
    "\n",
    "    data = data.copy()\n",
    "    \n",
    "    if not test:\n",
    "        data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50683cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename: str, test=False, cols=None):\n",
    "    \"\"\"\n",
    "    Load Agoda booking cancellation dataset\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        Path to house prices dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Design matrix and response vector in either of the following formats:\n",
    "    1) Single dataframe with last column representing the response\n",
    "    2) Tuple of pandas.DataFrame and Series\n",
    "    3) Tuple of ndarray of shape (n_samples, n_features) and ndarray of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    full_data = pd.read_csv(filename, parse_dates=DATETIME_COLS)\n",
    "    if test:\n",
    "        return parse_data(full_data, test, cols)\n",
    "    full_data = full_data.drop_duplicates()\n",
    "    parsed_data = parse_data(full_data)\n",
    "    features, labels = parsed_data.loc[:, parsed_data.columns != LABEL_COL], parsed_data[LABEL_COL]\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9eb6d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(extimator, X, y_true):\n",
    "    predictions = pd.DataFrame(estimator.predict(X))\n",
    "    conf_matrix = confusion_matrix(y_true, predictions)\n",
    "    print(conf_matrix)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, predictions)\n",
    "    print(\"Accuracy\", accuracy_score(y_true, predictions))\n",
    "    print(f\"True negative: {tn}, False positive: {fp}, False Negative: {fn}, True Positive: {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23dc2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle exporting data\n",
    "\n",
    "def evaluate_and_export(estimator, X: np.ndarray, filename: str):\n",
    "    \"\"\"\n",
    "    Export to specified file the prediction results of given estimator on given testset.\n",
    "\n",
    "    File saved is in csv format with a single column named 'predicted_values' and n_samples rows containing\n",
    "    predicted values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: BaseEstimator or any object implementing predict() method as in BaseEstimator (for example sklearn)\n",
    "        Fitted estimator to use for prediction\n",
    "\n",
    "    X: ndarray of shape (n_samples, n_features)\n",
    "        Test design matrix to predict its responses\n",
    "\n",
    "    filename:\n",
    "        path to store file at\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = pd.DataFrame(estimator.predict(X)).astype(int).to_numpy()\n",
    "    # print(predictions)\n",
    "    pd.DataFrame(predictions, columns=[\"predicted_values\"]).to_csv(filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8adbeb2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing training data...\n",
      "(43994, 525) (43994,)\n",
      "Fitting the model over the preprocesed training data...\n",
      "Loading and preprocessing the weekly test data (./test_set_week_3.csv)...\n",
      "(700, 525)\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noamw\\anaconda\\envs\\iml.env\\lib\\site-packages\\pandas\\core\\indexing.py:1667: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.obj[key] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13430    50]\n",
      " [ 1154    31]]\n",
      "Accuracy 0.9178997613365155\n",
      "True negative: 13430, False positive: 50, False Negative: 1154, True Positive: 31\n",
      "Predicting and exporting over the weekly test data...\n",
      "Done :)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    # Load data\n",
    "    print(\"Loading and preprocessing training data...\")\n",
    "    features, cancellation_labels = load_data(\"datasets/agoda_cancellation_train.csv\")\n",
    "\n",
    "    train_X, test_X, train_y, test_y = model_selection.train_test_split(features, cancellation_labels, random_state=0)\n",
    "    print(train_X.shape, train_y.shape)\n",
    "\n",
    "    # # Fit model over data\n",
    "    print(\"Fitting the model over the preprocesed training data...\")\n",
    "    estimator = LogisticRegression(solver='liblinear', max_iter=500)\n",
    "    estimator.fit(train_X, train_y)\n",
    "    \n",
    "    \n",
    "    print(f\"Loading and preprocessing the weekly test data ({WEEKLY_TEST_SET})...\")\n",
    "    parsed_data = load_data(WEEKLY_TEST_SET, test=True, cols=train_X.columns)\n",
    "    parsed_data = parsed_data.copy()\n",
    "    parsed_data = parsed_data.reindex(columns=parsed_data.columns, fill_value=0)\n",
    "    print(parsed_data.shape)\n",
    "    \n",
    "    # # # Store model predictions over test set\n",
    "    print(\"Evaluating...\")\n",
    "    evaluate(estimator, test_X, test_y)\n",
    "\n",
    "    #y_true_week1 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])    \n",
    "    #y_pred_week1 = pd.DataFrame(estimator.predict(parsed_data)).astype(int).to_numpy()\n",
    "    #print(\"Accuracy on test\", accuracy_score(y_pred_week1, y_true_week1))\n",
    "    \n",
    "    print(\"Predicting and exporting over the weekly test data...\")\n",
    "    evaluate_and_export(estimator, parsed_data, \"{}_{}_{}.csv\".format(*STUDENTS))\n",
    "    \n",
    "    print(\"Done :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04b6aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next things to try:\n",
    "# - Maybe try regular regression to predict the actual date of cancellation.\n",
    "# - Maybe try Deep Learning approach\n",
    "# - Try different approach to the cancel policy feature.\n",
    "# - Remove some uncommon categorical data, e.g: original_payment_policy=UAH - replace with \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5ff6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a29c6e8d4ec1f1f9dc241f87ab0a94b82fc3375eb3b0393c457d990823ac03ba"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
