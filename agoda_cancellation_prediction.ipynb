{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1410336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! In order for this file to run without any import errors and correct file paths:\n",
    "# the weekly test set .csv (`test_set_week_x.csv`) should be in a folder named 'weekly_test_sets' under 'IML.HUJI'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9eab1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and general configurations\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, f1_score\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "STUDENTS = ['313234940', '207906306', '204841423']\n",
    "\n",
    "week = 7\n",
    "WEEKLY_TEST_SET = os.path.join('weekly_test_sets', f'week_{week}_test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb2119f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data columns (by type)\n",
    "\n",
    "ID_COLS = ['h_booking_id', 'hotel_id', 'hotel_country_code', 'h_customer_id']\n",
    "\n",
    "DATETIME_COLS = ['booking_datetime', 'checkin_date', 'checkout_date', 'hotel_live_date']\n",
    "\n",
    "CODES_COLS = ['origin_country_code', 'hotel_area_code', 'hotel_city_code']\n",
    "\n",
    "# (dropped) 'hotel_brand_code', 'hotel_chain_code' (have ~43K nulls!)\n",
    "\n",
    "CATEGORICAL_COLS = ['accommadation_type_name', 'charge_option', 'customer_nationality',\n",
    "                    'guest_nationality_country_name', 'language', 'original_payment_method',\n",
    "                    'original_payment_type', 'original_payment_currency']\n",
    "\n",
    "NUMERICAL_COLS = ['hotel_star_rating', 'no_of_adults',  'no_of_children', 'no_of_extra_bed', 'no_of_room',\n",
    "                  'original_selling_amount']\n",
    "\n",
    "SHOULD_BE_BOOLEAN_COLS = ['guest_is_not_the_customer', 'request_nonesmoke', 'request_latecheckin',\n",
    "                          'request_highfloor', 'request_largebed', 'request_twinbeds', 'request_airport',\n",
    "                          'request_earlycheckin']\n",
    "\n",
    "# The following columns have 25040 nulls, we will treat null here as 0 (False):\n",
    "# request_nonesmoke, request_latecheckin, request_highfloor, request_largebed, request_twinbeds,\n",
    "# request_airport, request_earlycheckin\n",
    "# remove them - or fill in nan with 0\n",
    "\n",
    "BOOLEAN_COLS = ['is_user_logged_in', 'is_first_booking']\n",
    "\n",
    "LABEL_COL = 'cancellation_datetime'\n",
    "\n",
    "NO_SHOW_PATTERN = '_(\\d+)(N|P)'\n",
    "POLICY_PATTERN = '(\\d+)D(\\d+)(N|P)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d10a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data parsing functions\n",
    "\n",
    "def parse_all_cancellation_policy(data):\n",
    "    counter = 0\n",
    "    for i, row in data.iterrows():       \n",
    "        if row[\"cancellation_policy_code\"] == \"UNKNOWN\":\n",
    "            row[\"cancellation_policy_code\"] = '0D100P_100P'\n",
    "            counter += 1\n",
    "\n",
    "        policy = row[\"cancellation_policy_code\"]\n",
    "        n_nights = row[\"n_nights\"]\n",
    "\n",
    "        no_show = re.findall(NO_SHOW_PATTERN, policy)\n",
    "        cancel_policy = re.findall(POLICY_PATTERN, policy)\n",
    "        worse_policy = cancel_policy[-1]\n",
    "        basic_policy = cancel_policy[-2] if len(cancel_policy) > 1 else worse_policy\n",
    "        if no_show:\n",
    "            no_show_as_int = int(no_show[0][0])\n",
    "            if no_show[0][1] == \"N\":\n",
    "                no_show_days = no_show_as_int\n",
    "                no_show_percent = no_show_as_int / n_nights * 100\n",
    "            else:\n",
    "                no_show_days = no_show_as_int * n_nights / 100\n",
    "                no_show_percent = no_show_as_int\n",
    "\n",
    "        else:\n",
    "            worse_policy_without_no_show = re.findall(POLICY_PATTERN, policy)[-1]\n",
    "\n",
    "            if worse_policy_without_no_show[-1] == \"N\":\n",
    "                no_show_days = int(worse_policy_without_no_show[1])\n",
    "                no_show_percent = int(worse_policy_without_no_show[1]) / n_nights * 100\n",
    "            else:\n",
    "                no_show_days = int(worse_policy_without_no_show[1]) * n_nights / 100\n",
    "                no_show_percent = int(worse_policy_without_no_show[1])\n",
    "\n",
    "        \n",
    "        worse_policy_as_int = int(worse_policy[1])\n",
    "        basic_policy_as_int = int(basic_policy[1])\n",
    "        if worse_policy[2] == \"N\":\n",
    "            #nights = worse_policy_as_int\n",
    "            percent = worse_policy_as_int / n_nights * 100\n",
    "        else:\n",
    "            #nights = worse_policy_as_int * n_nights / 100\n",
    "            percent = worse_policy_as_int\n",
    "        if basic_policy[2] == \"N\":\n",
    "            #basic_by_nights = worse_policy_as_int\n",
    "            basic_percent = worse_policy_as_int / n_nights * 100\n",
    "        else:\n",
    "            #basic_by_nights = int(basic_policy_as_int) * n_nights / 100\n",
    "            basic_percent = int(basic_policy_as_int)\n",
    "        \n",
    "\n",
    "        data.loc[i, \"no_show_days\"] = no_show_days\n",
    "        data.loc[i, \"no_show_percentage\"] = no_show_percent\n",
    "\n",
    "        \n",
    "        days = int(worse_policy[0])\n",
    "        basic_days = int(basic_policy[0])\n",
    "        data.loc[i, \"basic_charge_percentage\"] = basic_percent\n",
    "        #data.loc[i, \"basic_charge_by_nights\"] = basic_by_nights\n",
    "        data.loc[i, \"basic_charge_days\"] = basic_days\n",
    "        #data.loc[i, \"basic_charge_days_times_nights\"] = basic_days * basic_by_nights\n",
    "        #data.loc[i, \"basic_charge_days_times_percentage\"] = basic_days * basic_percent\n",
    "        data.loc[i, \"charge_percentage\"] = percent\n",
    "        #data.loc[i, \"charge_by_nights\"] = nights\n",
    "        data.loc[i, \"charge_days\"] = days\n",
    "        #data.loc[i, \"charge_days_times_nights\"] = days * nights\n",
    "        #data.loc[i, \"charge_days_times_percentage\"] = days * percent\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_dates(data):\n",
    "    data = data.copy()\n",
    "    for col_name in DATETIME_COLS:\n",
    "        col_date_obj = data[col_name]\n",
    "        data[f\"{col_name}_year\"] = col_date_obj.dt.year\n",
    "        data[f\"{col_name}_month\"] = col_date_obj.dt.month\n",
    "        data[f\"{col_name}_day_in_week\"] = col_date_obj.dt.day_of_week\n",
    "        data = pd.get_dummies(data, columns=[f\"{col_name}_year\", f\"{col_name}_month\", f\"{col_name}_day_in_week\"], drop_first=True)\n",
    "        \n",
    "    data[\"n_nights\"] = (data[\"checkout_date\"] - data[\"checkin_date\"]).dt.days\n",
    "    data[\"n_days_from_booking_to_checkin\"] = (data[\"checkin_date\"] - data[\"booking_datetime\"]).dt.days\n",
    "\n",
    "    data = data.drop(DATETIME_COLS, axis=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbe7c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(full_data, test=False, cols=None):\n",
    "    # choose some of the original columns\n",
    "    if test:\n",
    "        data = full_data[NUMERICAL_COLS + DATETIME_COLS + SHOULD_BE_BOOLEAN_COLS + BOOLEAN_COLS +\n",
    "                         CATEGORICAL_COLS + [\"cancellation_policy_code\"]]\n",
    "    else:\n",
    "        data = full_data[NUMERICAL_COLS + DATETIME_COLS + SHOULD_BE_BOOLEAN_COLS + BOOLEAN_COLS +\n",
    "                         CATEGORICAL_COLS + [\"cancellation_policy_code\"] + [LABEL_COL]]\n",
    "\n",
    "    data = data.copy()\n",
    "        \n",
    "    # edit labels\n",
    "    if not test:\n",
    "        booking_date_diff = (pd.to_datetime(data[LABEL_COL]) - data[\"booking_datetime\"]).dt.days.between(7, 44) \n",
    "        checkin_diff = (pd.to_datetime(data[LABEL_COL]) - data[\"checkin_date\"]).dt.days.between(2, 9) \n",
    "        \n",
    "        data[LABEL_COL] = (booking_date_diff) | (checkin_diff)\n",
    "        data = data.copy()\n",
    "\n",
    "    # fill in 0 instead of None's\n",
    "    for col_name in SHOULD_BE_BOOLEAN_COLS:\n",
    "        data.loc[:, col_name] = data.loc[:, col_name].fillna(0)\n",
    "\n",
    "    # handle datetime cols\n",
    "    data = parse_dates(data)\n",
    "    \n",
    "    data = parse_all_cancellation_policy(data)\n",
    "    data = data.drop(['cancellation_policy_code'], axis=1)\n",
    "    \n",
    "    \n",
    "    for col_name in CATEGORICAL_COLS:\n",
    "        options = data[col_name].unique()\n",
    "        for option in options:\n",
    "            if len(data[data[col_name] == option]) / len(data) < 0.0005:\n",
    "                data.loc[data[col_name] == option, col_name] = \"OTHER\"\n",
    "    \n",
    "    # replace categorical features with their dummies\n",
    "    data = pd.get_dummies(data, columns=CATEGORICAL_COLS, drop_first=True)\n",
    "    \n",
    "    data = data.copy()\n",
    "    if cols is not None:\n",
    "        missing_cols = set(cols) - set(data.columns)\n",
    "        for col in missing_cols:\n",
    "            data.loc[:, col] = 0\n",
    "        data = data.loc[:, cols] \n",
    "\n",
    "    data = data.copy()\n",
    "    \n",
    "    if not test:\n",
    "        data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50683cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename: str, test=False, cols=None):\n",
    "    \"\"\"\n",
    "    Load Agoda booking cancellation dataset\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        Path to house prices dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Design matrix and response vector in either of the following formats:\n",
    "    1) Single dataframe with last column representing the response\n",
    "    2) Tuple of pandas.DataFrame and Series\n",
    "    3) Tuple of ndarray of shape (n_samples, n_features) and ndarray of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    original_train_data = pd.read_csv(filename, parse_dates=DATETIME_COLS)  \n",
    "\n",
    "    if test:\n",
    "        return parse_data(original_train_data, test, cols)\n",
    "    original_train_data = original_train_data.drop_duplicates()\n",
    "    parsed_data = parse_data(original_train_data)\n",
    "    features, labels = parsed_data.loc[:, parsed_data.columns != LABEL_COL], parsed_data[LABEL_COL]\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9eb6d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_cutoff(estimator, X, y_true):\n",
    "    predictions_proba = estimator.predict_proba(X)[:,1]\n",
    "    f1_scores = []\n",
    "    for cutoff in range(0, 100, 5):\n",
    "        cutoff /= 100\n",
    "        predictions = pd.DataFrame([1 if i > cutoff else 0 for i in predictions_proba])\n",
    "        f1_scores.append({\"cutoff\": cutoff, \"score\": f1_score(y_true, predictions, average='macro')})\n",
    "    return sorted(f1_scores, key=lambda d: d['score'])[-1][\"cutoff\"]\n",
    "    \n",
    "\n",
    "def evaluate(extimator, X, y_true):\n",
    "#     chosen_cutoff = choose_cutoff(estimator, X, y_true)\n",
    "#     print(\"chosen_cutoff\", chosen_cutoff)\n",
    "#     predictions = pd.DataFrame([1 if i > chosen_cutoff else 0 for i in  estimator.predict_proba(X)[:,1]])\n",
    "    \n",
    "    predictions = estimator.predict(X)\n",
    "    print(\"F1:\", f1_score(y_true, predictions, average='macro'))\n",
    "#     return chosen_cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23dc2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle exporting data\n",
    "\n",
    "def evaluate_and_export(estimator, cutoff, X: np.ndarray, filename: str):\n",
    "    \"\"\"\n",
    "    Export to specified file the prediction results of given estimator on given testset.\n",
    "\n",
    "    File saved is in csv format with a single column named 'predicted_values' and n_samples rows containing\n",
    "    predicted values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: BaseEstimator or any object implementing predict() method as in BaseEstimator (for example sklearn)\n",
    "        Fitted estimator to use for prediction\n",
    "\n",
    "    X: ndarray of shape (n_samples, n_features)\n",
    "        Test design matrix to predict its responses\n",
    "\n",
    "    filename:\n",
    "        path to store file at\n",
    "\n",
    "    \"\"\"\n",
    "#     predictions = pd.DataFrame([1 if i > cutoff else 0 for i in estimator.predict_proba(X)[:,1]]).to_numpy()\n",
    "    predictions = estimator.predict(X).astype(int)\n",
    "    pd.DataFrame(predictions, columns=[\"predicted_values\"]).to_csv(filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "844bc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def previous_test_sets(features_columns):\n",
    "    all_features = pd.DataFrame({}, columns=features_columns)\n",
    "    all_labels = pd.DataFrame({}, columns=['h_booking_id', 'cancel'])\n",
    "    \n",
    "    for week in [1, 2, 3, 4, 5, 6]:\n",
    "        testset_filename = os.path.join('weekly_test_sets', f'week_{week}_test_data.csv')\n",
    "        labels_filename = os.path.join('labels', f'week_{week}_labels.csv')\n",
    "        \n",
    "        features_week = load_data(testset_filename, test=True, cols=features_columns)\n",
    "        labels_week = pd.read_csv(labels_filename)\n",
    "        \n",
    "        all_features = pd.concat([all_features, features_week])\n",
    "        all_labels = pd.concat([all_labels, labels_week])\n",
    "        \n",
    "    return all_features, all_labels.rename({\"cancel\": LABEL_COL}, axis=1)[LABEL_COL].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8adbeb2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing training data...\n",
      "Fitting the model over the preprocesed training data...\n",
      "Loading and preprocessing the weekly test data (weekly_test_sets\\week_7_test_set.csv)...\n",
      "Evaluating...\n",
      "F1: 0.5426875485570273\n",
      "Predicting and exporting over the weekly test data...\n",
      "Done :)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "\n",
    "    print(\"Loading and preprocessing training data...\")\n",
    "    features, labels = load_data(\"datasets/agoda_cancellation_train.csv\")\n",
    "\n",
    "    prev_weeks_features, prev_weeks_labels = previous_test_sets(features.columns)\n",
    "\n",
    "#     all_features, all_labels = features, labels\n",
    "    all_features = pd.concat([features, prev_weeks_features])\n",
    "    all_labels = pd.concat([labels, prev_weeks_labels])\n",
    "    \n",
    "#     best_features = SelectKBest(k=100)\n",
    "        \n",
    "    train_X, test_X, train_y, test_y = model_selection.train_test_split(all_features, all_labels, random_state=0)\n",
    "\n",
    "    # Fit model over data\n",
    "    print(\"Fitting the model over the preprocesed training data...\")\n",
    "#     estimator = RandomForestClassifier().fit(all_features, all_labels)\n",
    "    estimator = RandomForestClassifier().fit(train_X, train_y)\n",
    "#     estimator = CatBoostClassifier(iterations=200).fit(train_X, train_y, verbose=False)\n",
    "    \n",
    "    print(f\"Loading and preprocessing the weekly test data ({WEEKLY_TEST_SET})...\")\n",
    "    parsed_data = load_data(WEEKLY_TEST_SET, test=True, cols=features.columns)\n",
    "    parsed_data = parsed_data.copy()\n",
    "    parsed_data = parsed_data.reindex(columns=parsed_data.columns, fill_value=0)\n",
    "    \n",
    "    print(\"Evaluating...\")\n",
    "#     chosen_cutoff = evaluate(estimator, test_X, test_y)\n",
    "    evaluate(estimator, test_X, test_y.astype(int))\n",
    "    chosen_cutoff = None\n",
    "     \n",
    "    print(\"Predicting and exporting over the weekly test data...\")\n",
    "    evaluate_and_export(estimator, chosen_cutoff, parsed_data, \"{}_{}_{}.csv\".format(*STUDENTS))\n",
    "    \n",
    "#     week = 5  # remember to remove it from the prev\n",
    "#     y_pred = pd.read_csv(\"313234940_207906306_204841423.csv\")[\"predicted_values\"]    \n",
    "#     y_true = pd.read_csv(os.path.join('labels', f'week_{week}_labels.csv')).rename({\"cancel\": LABEL_COL}, axis=1)[LABEL_COL].astype(int)\n",
    "#     print(f\"f1_score of week {week}:\", f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "    print(\"Done :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b6aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next things to try:\n",
    "# - Maybe try regular regression to predict the actual date of cancellation.\n",
    "# - Maybe try Deep Learning approach\n",
    "# - Try different approach to the cancel policy feature.\n",
    "# - Try different classifiers\n",
    "# - Try to understand how to make the model more sensitive (predict more Positives)\n",
    "# new:\n",
    "# - try to fit over the original set with the new weeks\n",
    "# - check if there's a cutoff in this model + other parameters we can config\n",
    "# - we can also try to use the BaggingReggresor wuth different base estimators (nead to convert the prediction to binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5ff6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a29c6e8d4ec1f1f9dc241f87ab0a94b82fc3375eb3b0393c457d990823ac03ba"
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-iml.env]",
   "language": "python",
   "name": "conda-env-anaconda3-iml.env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
