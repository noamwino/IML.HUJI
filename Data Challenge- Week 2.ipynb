{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1410336a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! In order for this file to run without any import errors, it needs to be directly within the \"IML.HUJI\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04b6aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things to try:\n",
    "# 1. labels are True only if the times passed from the booking date / checkindate to the cancellation matches the instructions.\n",
    "# 2. drop na completely from train (mainly in the requests section)\n",
    "# 3. Maybe try regular regression to predict the actual date of cancellation.\n",
    "# 4. Maybe try Deep Learning approach\n",
    "# 5. Try different approach to the cancel policy deature.\n",
    "# 6. Remove some uncommon categorical data, e.g: original_payment_policy=UAH - replace with \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9eab1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and general configurations\n",
    "\n",
    "# from challenge.agoda_cancellation_estimator import AgodaCancellationEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "STUDENTS = ['313234940', '207906306', '204841423']\n",
    "\n",
    "# The file should be in the same folder as this notebook\n",
    "WEEKLY_TEST_SET = './test_set_week_1.csv'\n",
    "# WEEKLY_TEST_SET = './test_set_week_2.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb2119f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data columns (by type)\n",
    "\n",
    "ID_COLS = ['h_booking_id', 'hotel_id', 'hotel_country_code', 'h_customer_id']\n",
    "\n",
    "DATETIME_COLS = ['booking_datetime', 'checkin_date', 'checkout_date', 'hotel_live_date']\n",
    "\n",
    "CODES_COLS = ['origin_country_code', 'hotel_area_code', 'hotel_city_code']\n",
    "\n",
    "# (dropped) 'hotel_brand_code', 'hotel_chain_code' (have ~43K nulls!)\n",
    "\n",
    "CATEGORICAL_COLS = ['accommadation_type_name', 'charge_option', 'customer_nationality',\n",
    "                    'guest_nationality_country_name', 'language', 'original_payment_method',\n",
    "                    'original_payment_type', 'original_payment_currency']\n",
    "\n",
    "NUMERICAL_COLS = ['hotel_star_rating', 'no_of_adults',  'no_of_children', 'no_of_extra_bed', 'no_of_room',\n",
    "                  'original_selling_amount']\n",
    "\n",
    "SHOULD_BE_BOOLEAN_COLS = ['guest_is_not_the_customer', 'request_nonesmoke', 'request_latecheckin',\n",
    "                          'request_highfloor', 'request_largebed', 'request_twinbeds', 'request_airport',\n",
    "                          'request_earlycheckin']\n",
    "\n",
    "# The following columns have 25040 nulls, we will treat null here as 0 (False):\n",
    "# request_nonesmoke, request_latecheckin, request_highfloor, request_largebed, request_twinbeds,\n",
    "# request_airport, request_earlycheckin\n",
    "# remove them - or fill in nan with 0\n",
    "\n",
    "BOOLEAN_COLS = ['is_user_logged_in', 'is_first_booking']\n",
    "\n",
    "LABEL_COL = 'cancellation_datetime'\n",
    "\n",
    "NO_SHOW_PATTERN = '_(\\d+)(N|P)'\n",
    "POLICY_PATTERN = '(\\d+)D(\\d+)(N|P)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d10a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data parsing functions\n",
    "\n",
    "def parse_all_cancellation_policy(data):\n",
    "    counter = 0\n",
    "    for i, row in data.iterrows():\n",
    "        #if i % 2000 == 0:\n",
    "        #    print(i)\n",
    "        \n",
    "        if row[\"cancellation_policy_code\"] == \"UNKNOWN\":\n",
    "            row[\"cancellation_policy_code\"] = '0D100P_100P'\n",
    "            counter += 1\n",
    "\n",
    "        policy = row[\"cancellation_policy_code\"]\n",
    "        n_nights = row[\"n_nights\"]\n",
    "\n",
    "        no_show = re.findall(NO_SHOW_PATTERN, policy)\n",
    "        cancel_policy = re.findall(POLICY_PATTERN, policy)\n",
    "        worse_policy = cancel_policy[-1]\n",
    "        basic_policy = cancel_policy[-2] if len(cancel_policy) > 1 else worse_policy\n",
    "        if no_show:\n",
    "            no_show_as_int = int(no_show[0][0])\n",
    "            if no_show[0][1] == \"N\":\n",
    "                no_show_days = no_show_as_int\n",
    "                no_show_percent = no_show_as_int / n_nights * 100\n",
    "            else:\n",
    "                no_show_days = no_show_as_int * n_nights / 100\n",
    "                no_show_percent = no_show_as_int\n",
    "\n",
    "        else:\n",
    "            worse_policy_without_no_show = re.findall(POLICY_PATTERN, policy)[-1]\n",
    "\n",
    "            if worse_policy_without_no_show[-1] == \"N\":\n",
    "                no_show_days = int(worse_policy_without_no_show[1])\n",
    "                no_show_percent = int(worse_policy_without_no_show[1]) / n_nights * 100\n",
    "            else:\n",
    "                no_show_days = int(worse_policy_without_no_show[1]) * n_nights / 100\n",
    "                no_show_percent = int(worse_policy_without_no_show[1])\n",
    "\n",
    "        \n",
    "        worse_policy_as_int = int(worse_policy[1])\n",
    "        basic_policy_as_int = int(basic_policy[1])\n",
    "        if worse_policy[2] == \"N\":\n",
    "            #nights = worse_policy_as_int\n",
    "            percent = worse_policy_as_int / n_nights * 100\n",
    "        else:\n",
    "            #nights = worse_policy_as_int * n_nights / 100\n",
    "            percent = worse_policy_as_int\n",
    "        if basic_policy[2] == \"N\":\n",
    "            #basic_by_nights = worse_policy_as_int\n",
    "            basic_percent = worse_policy_as_int / n_nights * 100\n",
    "        else:\n",
    "            #basic_by_nights = int(basic_policy_as_int) * n_nights / 100\n",
    "            basic_percent = int(basic_policy_as_int)\n",
    "        \n",
    "\n",
    "        data.loc[i, \"no_show_days\"] = no_show_days\n",
    "        data.loc[i, \"no_show_percentage\"] = no_show_percent\n",
    "\n",
    "        \n",
    "        days = int(worse_policy[0])\n",
    "        basic_days = int(basic_policy[0])\n",
    "        data.loc[i, \"basic_charge_percentage\"] = basic_percent\n",
    "        #data.loc[i, \"basic_charge_by_nights\"] = basic_by_nights\n",
    "        data.loc[i, \"basic_charge_days\"] = basic_days\n",
    "        #data.loc[i, \"basic_charge_days_times_nights\"] = basic_days * basic_by_nights\n",
    "        #data.loc[i, \"basic_charge_days_times_percentage\"] = basic_days * basic_percent\n",
    "        data.loc[i, \"charge_percentage\"] = percent\n",
    "        #data.loc[i, \"charge_by_nights\"] = nights\n",
    "        data.loc[i, \"charge_days\"] = days\n",
    "        #data.loc[i, \"charge_days_times_nights\"] = days * nights\n",
    "        #data.loc[i, \"charge_days_times_percentage\"] = days * percent\n",
    "\n",
    "    return data\n",
    "\n",
    "def parse_dates(data):\n",
    "    data = data.copy()\n",
    "    for col_name in DATETIME_COLS:\n",
    "        col_date_obj = data[col_name]\n",
    "        data[f\"{col_name}_year\"] = col_date_obj.dt.year\n",
    "        data[f\"{col_name}_month\"] = col_date_obj.dt.month\n",
    "        data[f\"{col_name}_day_in_week\"] = col_date_obj.dt.day_of_week\n",
    "        #data[f\"{col_name}_dayofyear\"] = col_date_obj.dt.dayofyear\n",
    "        data = pd.get_dummies(data, columns=[f\"{col_name}_year\", f\"{col_name}_month\", f\"{col_name}_day_in_week\"], drop_first=True)\n",
    "        \n",
    "    data[\"n_nights\"] = (data[\"checkout_date\"] - data[\"checkin_date\"]).dt.days\n",
    "    data[\"n_days_from_booking_to_checkin\"] = (data[\"checkin_date\"] - data[\"booking_datetime\"]).dt.days\n",
    "\n",
    "    data = data.drop(DATETIME_COLS, axis=1)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cbe7c304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(full_data, test=False, cols=None):\n",
    "    # choose some of the original columns\n",
    "    if test:\n",
    "        data = full_data[NUMERICAL_COLS + DATETIME_COLS + SHOULD_BE_BOOLEAN_COLS + BOOLEAN_COLS +\n",
    "                         CATEGORICAL_COLS + [\"cancellation_policy_code\"]]\n",
    "    else:\n",
    "        data = full_data[NUMERICAL_COLS + DATETIME_COLS + SHOULD_BE_BOOLEAN_COLS + BOOLEAN_COLS +\n",
    "                         CATEGORICAL_COLS + [\"cancellation_policy_code\"] + [LABEL_COL]]\n",
    "\n",
    "    data = data.copy()\n",
    "        \n",
    "    # edit labels\n",
    "    if not test:\n",
    "        #data[LABEL_COL] = data[LABEL_COL].notnull()  # todo maybe change to handle the dates?\n",
    "        booking_date_diff = (pd.to_datetime(data[LABEL_COL]) - data[\"booking_datetime\"]).dt.days.between(7, 44) \n",
    "        checkin_diff = (pd.to_datetime(data[LABEL_COL]) - data[\"checkin_date\"]).dt.days.between(2, 9) \n",
    "        \n",
    "        data[LABEL_COL] = (booking_date_diff) | (checkin_diff)\n",
    "        data = data.copy()\n",
    "\n",
    "    # fill in 0 instead of None's\n",
    "    for col_name in SHOULD_BE_BOOLEAN_COLS:\n",
    "        data.loc[:, col_name] = data.loc[:, col_name].fillna(0)\n",
    "\n",
    "    # handle datetime cols\n",
    "    data = parse_dates(data)\n",
    "    \n",
    "    data = parse_all_cancellation_policy(data)\n",
    "    data = data.drop(['cancellation_policy_code'], axis=1)\n",
    "    \n",
    "    # replace categorical features with their dummies\n",
    "    data = pd.get_dummies(data, columns=CATEGORICAL_COLS, drop_first=True)\n",
    "    \n",
    "    data = data.copy()\n",
    "    if cols is not None:\n",
    "        missing_cols = set(cols) - set(data.columns)\n",
    "        for col in missing_cols:\n",
    "            data.loc[:, col] = 0\n",
    "        data = data.loc[:, cols] \n",
    "\n",
    "    data = data.copy()\n",
    "    \n",
    "    if not test:\n",
    "        data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50683cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename: str, test=False, cols=None):\n",
    "    \"\"\"\n",
    "    Load Agoda booking cancellation dataset\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        Path to house prices dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Design matrix and response vector in either of the following formats:\n",
    "    1) Single dataframe with last column representing the response\n",
    "    2) Tuple of pandas.DataFrame and Series\n",
    "    3) Tuple of ndarray of shape (n_samples, n_features) and ndarray of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    full_data = pd.read_csv(filename, parse_dates=DATETIME_COLS)\n",
    "    if test:\n",
    "        return parse_data(full_data, test, cols)\n",
    "    full_data = full_data.drop_duplicates()\n",
    "    parsed_data = parse_data(full_data)\n",
    "    features, labels = parsed_data.loc[:, parsed_data.columns != LABEL_COL], parsed_data[LABEL_COL]\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9eb6d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(extimator, X, y_true):\n",
    "    predictions = pd.DataFrame(estimator.predict(X))\n",
    "    conf_matrix = confusion_matrix(y_true, predictions)\n",
    "    print(conf_matrix)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_true, predictions)\n",
    "    print(\"Accuracy\", accuracy_score(y_true, predictions))\n",
    "    print(\"AUC\", auc(fpr, tpr))\n",
    "    print(f\"True negative: {tn}, False positive: {fp}, False Negative: {fn}, True Positive: {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23dc2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle exporting data\n",
    "\n",
    "def evaluate_and_export(estimator, X: np.ndarray, filename: str):\n",
    "    \"\"\"\n",
    "    Export to specified file the prediction results of given estimator on given testset.\n",
    "\n",
    "    File saved is in csv format with a single column named 'predicted_values' and n_samples rows containing\n",
    "    predicted values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: BaseEstimator or any object implementing predict() method as in BaseEstimator (for example sklearn)\n",
    "        Fitted estimator to use for prediction\n",
    "\n",
    "    X: ndarray of shape (n_samples, n_features)\n",
    "        Test design matrix to predict its responses\n",
    "\n",
    "    filename:\n",
    "        path to store file at\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = pd.DataFrame(estimator.predict(X)).astype(int).to_numpy()\n",
    "    print(predictions)\n",
    "    # pd.DataFrame(predictions, columns=[\"predicted_values\"]).to_csv(filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8adbeb2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing training data...\n",
      "Fitting the model over the preprocesed training data...\n",
      "Fitting:\n",
      "Index(['hotel_star_rating', 'no_of_adults', 'no_of_children',\n",
      "       'no_of_extra_bed', 'no_of_room', 'original_selling_amount',\n",
      "       'guest_is_not_the_customer', 'request_nonesmoke', 'request_latecheckin',\n",
      "       'request_highfloor',\n",
      "       ...\n",
      "       'original_payment_currency_SEK', 'original_payment_currency_SGD',\n",
      "       'original_payment_currency_THB', 'original_payment_currency_TRY',\n",
      "       'original_payment_currency_TWD', 'original_payment_currency_UAH',\n",
      "       'original_payment_currency_USD', 'original_payment_currency_VND',\n",
      "       'original_payment_currency_XPF', 'original_payment_currency_ZAR'],\n",
      "      dtype='object', length=531)\n",
      "[[-5.17882126e-02  2.61482815e-02  6.63915775e-03 -1.40436403e-01\n",
      "  -6.40498158e-02  3.62899368e-05 -2.97856979e-02  8.02810139e-02\n",
      "   1.92254300e-01 -1.35278387e-01 -2.27796219e-01 -1.27314512e-01\n",
      "  -7.76563208e-01  8.92118686e-02  1.49645801e-01  1.07272618e-01\n",
      "   6.94725689e-01 -4.73436160e-02  3.63588905e-01  6.74575490e-01\n",
      "   1.05659966e+00  1.28027441e+00  9.62396961e-01  8.18771352e-01\n",
      "  -3.93504402e-02 -1.13368076e+00 -1.09750782e+00  3.17739813e-01\n",
      "   1.47247658e-02  3.22646140e-02  5.67611283e-02  1.09689077e-01\n",
      "  -4.45522642e-02  8.52129526e-02 -2.64762171e-01 -2.44522476e-01\n",
      "  -8.62524378e-02 -4.82956365e-02 -1.29804264e-01 -4.08031771e-02\n",
      "  -2.68370386e-01 -8.72851297e-02  6.38505364e-02  4.76943354e-02\n",
      "   2.97990137e-01  6.54361255e-02 -2.03999531e-02  4.81588643e-02\n",
      "   3.18635795e-02  9.07341719e-02  1.02484330e-01  1.11836784e+00\n",
      "  -2.39883123e-01 -4.89184814e-01 -4.37727541e-01 -4.74378666e-01\n",
      "  -4.91487744e-01 -4.40758637e-01 -4.29988561e-01 -3.22616325e-01\n",
      "  -1.49056939e-01 -3.42734711e-01 -3.21104903e-02  5.61868218e-02\n",
      "  -1.62886609e-02 -7.62818746e-02  8.28829917e-03 -2.75921089e-01\n",
      "  -4.21662309e-02 -5.22722322e-02 -6.00735794e-02  5.41306605e-02\n",
      "   1.89649381e-01  4.23152656e-02 -1.14477916e-01 -1.93829351e-01\n",
      "  -5.03158998e-02 -8.96648255e-02 -8.76364967e-02 -8.11711803e-02\n",
      "   2.96047333e-02  7.69874732e-03  4.51653071e-02 -4.82676808e-03\n",
      "  -6.17641343e-05 -6.19347998e-03  9.96847726e-04  1.93459418e-03\n",
      "  -5.45106725e-01 -1.15088510e-02 -5.63323702e-01  4.47163949e-01\n",
      "  -1.41129506e-01 -3.92880263e-01 -6.33452815e-01 -7.74904463e-02\n",
      "  -8.07710841e-02 -1.21361651e-01 -8.67417692e-02 -4.26157318e-01\n",
      "   1.02496621e+00 -8.35978359e-02 -7.21538482e-01 -2.03486187e-01\n",
      "  -1.13911532e-01 -4.61030854e-01 -2.75884224e-01 -3.79089160e-01\n",
      "   2.32496367e-01 -9.59803018e-01  4.64244124e-02  4.16096746e-01\n",
      "   2.36577882e-01  1.16382989e+00 -7.78773996e-01 -1.36933807e-01\n",
      "  -6.99260158e-01 -4.07622475e-02 -7.03216600e-02 -3.03165721e-01\n",
      "  -3.19872837e-01  0.00000000e+00 -1.02937014e-01  5.12595723e-01\n",
      "  -1.17304520e-02  0.00000000e+00  2.79582974e-01 -6.69355244e-01\n",
      "  -4.03641998e-01 -1.91469736e-01 -7.69415184e-02  2.01158104e-01\n",
      "  -4.66388597e-02 -6.75969349e-01 -8.04829102e-01  6.37673852e-01\n",
      "  -9.48600614e-03  6.45306420e-01 -4.22982710e-01 -7.94448435e-03\n",
      "   0.00000000e+00 -2.35449197e-01  3.48146815e-01 -2.33095736e-01\n",
      "  -8.71704409e-01  8.94878926e-02 -3.18068317e-01 -5.07314558e-02\n",
      "  -1.77898352e-01  4.58008726e-02 -1.75518135e-01 -3.27122689e-02\n",
      "  -1.15223347e-01 -2.39563588e-02 -4.86530654e-01 -2.68309799e-02\n",
      "  -3.64538047e-01  1.36523360e-02 -5.47039492e-03 -5.22738226e-03\n",
      "  -1.82049284e-01  4.36637081e-01  4.81121517e-01 -7.08593395e-03\n",
      "   4.56371755e-02 -4.94421837e-01 -1.78928391e-01 -6.95096715e-01\n",
      "   4.25949776e-01 -7.45808345e-02 -1.02530266e-01 -1.33768062e-01\n",
      "   2.71111513e-01 -3.55031478e-01  1.16831613e-01  7.87122513e-01\n",
      "  -5.31055400e-01 -5.39400586e-02  0.00000000e+00 -6.06925153e-01\n",
      "  -3.99550076e-02 -1.71300857e-01  0.00000000e+00 -2.77264178e-01\n",
      "  -7.37602844e-02  7.99168772e-01  0.00000000e+00 -1.05156275e-01\n",
      "  -6.77865720e-02 -2.46090721e-02  4.01640419e-01  9.06449116e-01\n",
      "  -6.16978308e-01 -1.46789690e-01 -1.09478867e-01 -4.83250382e-02\n",
      "   5.88874640e-01 -7.26159055e-01  0.00000000e+00  5.94032187e-01\n",
      "  -1.12917451e-01  4.88282502e-01  4.40265562e-01 -3.82884767e-02\n",
      "  -5.43776019e-01  2.97500652e-01  1.31635858e-01 -2.38284270e-01\n",
      "  -3.01802536e-01  5.14605397e-01  4.76004614e-01 -4.29788153e-01\n",
      "   1.33831382e-01 -3.15121021e-01  5.97694360e-01 -1.61843942e-01\n",
      "   1.65709902e-01  1.01767665e+00 -6.22857050e-03  1.73147436e-01\n",
      "  -2.67416978e-01 -1.88188368e-01 -2.22281748e-01  0.00000000e+00\n",
      "  -3.19960782e-01  4.17414426e-01 -7.95842408e-01  1.52054253e-01\n",
      "  -1.15875658e-01  2.08370045e-02  4.23867476e-01 -5.99564988e-01\n",
      "  -5.13055368e-02  1.45348427e-01 -1.73763506e-02  9.03289745e-01\n",
      "   3.17230220e-01  5.09731841e-01 -7.28549361e-02  6.23802910e-01\n",
      "   5.01281506e-01 -5.07809130e-01  9.12433454e-02 -8.52256185e-01\n",
      "  -3.06739823e-01 -7.14835247e-02 -1.30683620e-01 -1.67511967e-01\n",
      "  -6.32120037e-02  1.86802284e-01  5.28997317e-01  7.01765370e-01\n",
      "  -5.52826528e-03  1.01570274e+00 -6.98777157e-01  0.00000000e+00\n",
      "  -2.86856705e-01  7.63280628e-01 -4.07622475e-02 -7.03216600e-02\n",
      "   5.46062477e-02 -7.61307373e-01 -8.69070456e-03 -4.70747446e-02\n",
      "  -9.63620710e-01  6.77182172e-01 -2.22145829e-01  2.79582974e-01\n",
      "  -8.54123674e-01  0.00000000e+00 -2.07493676e-01 -2.47596360e-01\n",
      "  -7.69415184e-02  2.80416500e-01 -4.11427653e-02  2.63481413e-01\n",
      "   0.00000000e+00 -8.04829102e-01 -3.51600671e-01 -3.63281078e-02\n",
      "   6.45306420e-01 -4.22982710e-01 -7.94448435e-03  0.00000000e+00\n",
      "   5.60461789e-01  6.07537670e-01 -2.33095736e-01  2.90277746e-01\n",
      "   1.49675872e-02 -3.18068317e-01  0.00000000e+00 -5.07314558e-02\n",
      "  -1.80120212e-01 -8.75297295e-02 -8.34666654e-01 -1.75236709e-02\n",
      "  -3.27122689e-02  0.00000000e+00 -1.15223347e-01 -2.39563588e-02\n",
      "  -3.56597301e-01 -3.05998146e-01  9.93927789e-02  4.55877763e-02\n",
      "  -5.47039492e-03 -1.82049284e-01  6.91909071e-02  3.35801319e-01\n",
      "  -2.69548487e-02 -1.93257884e-01 -1.93924101e-02  3.34734512e-01\n",
      "  -5.58676206e-01  4.25949776e-01  1.13007075e-01 -6.11444004e-01\n",
      "  -2.61724279e-01 -3.55031478e-01  1.22668858e-01  7.87122513e-01\n",
      "   2.54336261e-01  1.13376056e-01  0.00000000e+00 -7.05114445e-01\n",
      "  -3.99550076e-02 -2.65364663e-01 -2.84528735e-01 -7.37602844e-02\n",
      "  -3.58486234e-02  0.00000000e+00 -6.22941444e-01 -6.36957418e-02\n",
      "  -2.46090721e-02  4.01640419e-01  8.82941418e-01  4.47085739e-01\n",
      "  -1.46789690e-01 -1.09478867e-01 -6.45422944e-02  4.93286497e-01\n",
      "  -3.85804227e-02 -9.60489690e-01  0.00000000e+00  5.57991884e-01\n",
      "  -2.69613060e-01 -5.16538719e-02 -5.75896802e-01  1.13559254e+00\n",
      "  -3.82884767e-02 -3.18660342e-01 -3.97167908e-01  8.66860766e-01\n",
      "  -2.38284270e-01 -4.34216692e-03  3.41850756e-01 -4.46356583e-02\n",
      "   4.25430283e-02  1.21337244e-01 -3.15121021e-01 -9.05352966e-02\n",
      "  -1.61843942e-01  1.65709902e-01 -4.65652822e-01  2.98152102e-01\n",
      "  -1.90024684e-02 -5.84028278e-02 -4.88439867e-02 -4.24038422e-02\n",
      "   5.76566650e-01  0.00000000e+00  3.11626576e-01 -1.62677488e-01\n",
      "  -8.45374707e-02 -6.03124210e-01 -3.28042845e-01  4.96176944e-02\n",
      "   2.43771825e-01 -1.11652010e-01  1.71796695e-01  1.45348427e-01\n",
      "  -1.73763506e-02  3.82415729e-01 -1.20576070e-01  4.05527980e-01\n",
      "  -7.28549361e-02  3.76378691e-01 -1.92306983e-01 -1.15500625e-01\n",
      "  -2.15052255e-01 -8.52256185e-01 -3.06739823e-01 -7.14835247e-02\n",
      "   1.94618390e-02 -1.71716487e-01 -6.32120037e-02  1.86802284e-01\n",
      "  -2.00170866e-02  3.01234053e-01 -7.94448435e-03  1.14735572e-02\n",
      "  -8.03088701e-01 -4.13199149e-01 -3.43281976e-01 -2.80503788e-01\n",
      "  -3.25326835e-01 -2.62499569e-01  2.75848606e-01  7.14107736e-01\n",
      "  -4.18162346e-01 -8.85182331e-02 -3.07428321e-01 -8.21797245e-02\n",
      "   0.00000000e+00  4.92827207e-01  1.78701678e-01 -7.38091913e-01\n",
      "  -4.99318858e-01 -6.18418173e-01 -1.68351541e-01 -1.17195104e-01\n",
      "   3.22530251e-01 -6.18656472e-01 -6.62250403e-01 -2.11611098e-01\n",
      "  -2.64290303e-01  3.65451624e-01  3.93124932e-01 -4.17799275e-01\n",
      "  -8.23671834e-01 -8.74559605e-01  8.26753258e-02 -1.79872752e-01\n",
      "  -4.19515711e-01 -2.22281748e-01  5.62330625e-01 -1.68016418e-01\n",
      "  -9.06219511e-01  2.55434276e-01 -6.46888310e-01 -4.48362128e-01\n",
      "  -2.00482384e-01  4.71273667e-01 -3.59397297e-01 -2.35228273e-01\n",
      "  -5.82866056e-01 -1.27172165e-02 -2.87096451e-01  3.45689749e-02\n",
      "  -1.55781414e-01  6.52662957e-01 -2.03948032e-01 -7.93998332e-02\n",
      "  -8.77396876e-02 -2.24427269e-01 -3.01616851e-02 -4.83991964e-01\n",
      "  -1.55368053e-01 -9.25431504e-02 -3.35688748e-01 -3.24198848e-01\n",
      "  -4.50530578e-01  7.17916458e-01 -9.70631668e-01  7.90478089e-02\n",
      "   6.08413583e-01 -1.31300237e+00  7.59870878e-01 -7.70385934e-01\n",
      "  -1.54375898e-02 -1.61057306e-01 -2.70255469e-01 -1.79476300e-01\n",
      "   0.00000000e+00  1.76203926e+00  1.47216946e-01 -1.72586873e-01\n",
      "   5.17407250e-01 -4.06062306e-01 -6.32908254e-02 -1.25438833e+00\n",
      "  -1.89144666e+00 -1.89179954e-01 -3.18955034e-01  1.15848438e+00\n",
      "   2.19718061e-02 -8.57282700e-02  3.38007517e-01 -1.37056188e+00\n",
      "   8.56274918e-02  1.83567400e-01  1.99759425e-01  1.38240534e-01\n",
      "   2.93657237e-02 -3.99570104e-01  6.98517269e-01 -3.57319154e-01\n",
      "  -1.59264830e-01  4.29500550e-02 -2.60080191e-01  4.08141072e-01\n",
      "   7.69600919e-01 -2.95264912e-03 -2.57562534e-02 -5.46986122e-02\n",
      "   5.45758845e-02  2.78321216e-02 -1.05106800e-02 -1.08519619e+00\n",
      "   7.32899804e-01  3.01942086e-01  4.64100079e-01 -5.69725718e-02\n",
      "  -2.80352373e-01  5.07221697e-01 -2.97909888e-01  3.51459764e-01\n",
      "  -2.90098659e-01 -6.32108052e-01 -7.49785448e-02 -2.82454042e-01\n",
      "  -1.55407464e-01 -4.70123454e-01  3.81444102e-02  1.98108565e-01\n",
      "  -2.67107080e-01 -1.96188735e-01 -7.12398116e-01  1.13355937e-01\n",
      "  -2.64632373e-03 -5.16538719e-02  5.22980371e-01]]\n",
      "Loading and preprocessing the weekly test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noamw\\anaconda\\envs\\iml.env\\lib\\site-packages\\pandas\\core\\indexing.py:1667: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self.obj[key] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "[[13434    46]\n",
      " [ 1152    33]]\n",
      "Accuracy 0.918308898738493\n",
      "AUC 0.5122178191789054\n",
      "True negative: 13434, False positive: 46, False Negative: 1152, True Positive: 33\n",
      "Accuracy on test 0.9742857142857143\n",
      "Done :)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    # Load data\n",
    "    print(\"Loading and preprocessing training data...\")\n",
    "    df, cancellation_labels = load_data(\"datasets/agoda_cancellation_train.csv\")\n",
    "\n",
    "    train_X, test_X, train_y, test_y = model_selection.train_test_split(df, cancellation_labels, random_state=0)\n",
    "\n",
    "    # # Fit model over data\n",
    "    print(\"Fitting the model over the preprocesed training data...\")\n",
    "    estimator = LogisticRegression(solver='liblinear', max_iter=500)\n",
    "    estimator.fit(train_X, train_y)\n",
    "    \n",
    "    print(\"Fitting:\")\n",
    "    print(train_X.columns)\n",
    "    print(estimator.coef_)\n",
    "    \n",
    "    \n",
    "    print(\"Loading and preprocessing the weekly test data...\")\n",
    "    parsed_data = load_data(WEEKLY_TEST_SET, test=True, cols=train_X.columns)\n",
    "    parsed_data = parsed_data.copy()\n",
    "    parsed_data = parsed_data.reindex(columns=parsed_data.columns, fill_value=0)\n",
    "    \n",
    "    # # # Store model predictions over test set\n",
    "    print(\"Evaluating...\")\n",
    "    evaluate(estimator, test_X, test_y)\n",
    "\n",
    "    \n",
    "    y_true_week1 = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])    \n",
    "    y_pred_week1 = pd.DataFrame(estimator.predict(parsed_data)).astype(int).to_numpy()\n",
    "    print(\"Accuracy on test\", accuracy_score(y_pred_week1, y_true_week1))\n",
    "    \n",
    "\n",
    "    #print(\"Predicting and exporting over the weekly test data...\")\n",
    "    #evaluate_and_export(estimator, parsed_data, \"{}_{}_{}.csv\".format(*STUDENTS))\n",
    "    \n",
    "    print(\"Done :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5ff6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a29c6e8d4ec1f1f9dc241f87ab0a94b82fc3375eb3b0393c457d990823ac03ba"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
