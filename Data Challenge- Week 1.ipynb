{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f8d7a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important! In order for this file to run without any import errors, it needs to be directly within the \"IML.HUJI\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1d51b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Estimator class\n",
    "\n",
    "from __future__ import annotations\n",
    "from typing import NoReturn\n",
    "from IMLearn.base import BaseEstimator\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "class AgodaCancellationEstimator(BaseEstimator):\n",
    "    \"\"\"\n",
    "    An estimator for solving the Agoda Cancellation challenge\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> AgodaCancellationEstimator:\n",
    "        \"\"\"\n",
    "        Instantiate an estimator for solving the Agoda Cancellation challenge\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self.logistic_regression_obj = LogisticRegression(solver='liblinear', max_iter=500)\n",
    "        self.fitted = False\n",
    "\n",
    "    def _fit(self, X: np.ndarray, y: np.ndarray) -> NoReturn:\n",
    "        \"\"\"\n",
    "        Fit an estimator for given samples\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input data to fit an estimator for\n",
    "\n",
    "        y : ndarray of shape (n_samples, )\n",
    "            Responses of input data to fit to\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = self.logistic_regression_obj.fit(X, y)\n",
    "        self.coef_ = self.model.coef_\n",
    "        self.intercept_ = self.model.intercept_\n",
    "        self.fitted_ = True\n",
    "\n",
    "    def _predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict responses for given samples using fitted estimator\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Input data to predict responses for\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        responses : ndarray of shape (n_samples, )\n",
    "            Predicted responses of given samples\n",
    "        \"\"\"\n",
    "        return self.logistic_regression_obj.predict(X)\n",
    "\n",
    "    def _loss(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate performance under loss function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : ndarray of shape (n_samples, n_features)\n",
    "            Test samples\n",
    "\n",
    "        y : ndarray of shape (n_samples, )\n",
    "            True labels of test samples\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            Performance under loss function\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f9eab1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and general configurations\n",
    "\n",
    "# from challenge.agoda_cancellation_estimator import AgodaCancellationEstimator\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "STUDENTS = ['313234940', '207906306', '204841423']\n",
    "\n",
    "# The file should be in the same folder as this notebook\n",
    "WEEKLY_TEST_SET = './test_set_week_1.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cb2119f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data columns (by type)\n",
    "\n",
    "ID_COLS = ['h_booking_id', 'hotel_id', 'hotel_country_code', 'h_customer_id']\n",
    "\n",
    "DATETIME_COLS = ['booking_datetime', 'checkin_date', 'checkout_date', 'hotel_live_date']\n",
    "\n",
    "CODES_COLS = ['origin_country_code', 'hotel_area_code', 'hotel_city_code']\n",
    "\n",
    "# (dropped) 'hotel_brand_code', 'hotel_chain_code' (have ~43K nulls!)\n",
    "\n",
    "CATEGORICAL_COLS = ['accommadation_type_name', 'charge_option', 'customer_nationality',\n",
    "                    'guest_nationality_country_name', 'language', 'original_payment_method',\n",
    "                    'original_payment_type', 'original_payment_currency']\n",
    "\n",
    "NUMERICAL_COLS = ['hotel_star_rating', 'no_of_adults',  'no_of_children', 'no_of_extra_bed', 'no_of_room',\n",
    "                  'original_selling_amount']\n",
    "\n",
    "SHOULD_BE_BOOLEAN_COLS = ['guest_is_not_the_customer', 'request_nonesmoke', 'request_latecheckin',\n",
    "                          'request_highfloor', 'request_largebed', 'request_twinbeds', 'request_airport',\n",
    "                          'request_earlycheckin']\n",
    "\n",
    "# The following columns have 25040 nulls:\n",
    "# request_nonesmoke, request_latecheckin, request_highfloor, request_largebed, request_twinbeds,\n",
    "# request_airport, request_earlycheckin\n",
    "# remove them - or fill in nan with 0\n",
    "\n",
    "BOOLEAN_COLS = ['is_user_logged_in', 'is_first_booking']\n",
    "\n",
    "LABEL_COL = 'cancellation_datetime'\n",
    "\n",
    "NO_SHOW_PATTERN = '_(\\d+)(N|P)'\n",
    "POLICY_PATTERN = '(\\d+)D(\\d+)(N|P)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0d10a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data parsing functions\n",
    "\n",
    "def parse_all_cancellation_policy(data):\n",
    "    counter = 0\n",
    "    for i, row in data.iterrows():\n",
    "#         if i % 2000 == 0:\n",
    "#             print(i)\n",
    "        \n",
    "        if row[\"cancellation_policy_code\"] == \"UNKNOWN\":\n",
    "            row[\"cancellation_policy_code\"] = '0D100P_100P'\n",
    "            counter += 1\n",
    "\n",
    "        policy = row[\"cancellation_policy_code\"]\n",
    "        n_nights = row[\"n_nights\"]\n",
    "\n",
    "        no_show = re.findall(NO_SHOW_PATTERN, policy)\n",
    "        cancel_policy = re.findall(POLICY_PATTERN, policy)\n",
    "        worse_policy = cancel_policy[-1]\n",
    "        basic_policy = cancel_policy[-2] if len(cancel_policy) > 1 else worse_policy\n",
    "        if no_show:\n",
    "            if no_show[0][1] == \"N\":\n",
    "                no_show_days = int(no_show[0][0])\n",
    "                no_show_percent = int(no_show[0][0]) / n_nights * 100\n",
    "            else:\n",
    "                no_show_days = int(no_show[0][0]) * n_nights / 100\n",
    "                no_show_percent = int(no_show[0][0])\n",
    "\n",
    "        else:\n",
    "            worse_policy_without_no_show = re.findall(POLICY_PATTERN, policy)[-1]\n",
    "\n",
    "            if worse_policy_without_no_show[-1] == \"N\":\n",
    "                no_show_days = int(worse_policy_without_no_show[1])\n",
    "                no_show_percent = int(worse_policy_without_no_show[1]) / n_nights * 100\n",
    "            else:\n",
    "                no_show_days = int(worse_policy_without_no_show[1]) * n_nights / 100\n",
    "                no_show_percent = int(worse_policy_without_no_show[1])\n",
    "\n",
    "        if worse_policy[2] == \"N\":\n",
    "            nights = int(worse_policy[1])\n",
    "            percent = int(worse_policy[1]) / n_nights * 100\n",
    "        else:\n",
    "            nights = int(worse_policy[1]) * n_nights / 100\n",
    "            percent = int(worse_policy[1])\n",
    "        if basic_policy[2] == \"N\":\n",
    "            basic_by_nights = int(basic_policy[1])\n",
    "            basic_percent = int(basic_policy[1]) / n_nights * 100\n",
    "        else:\n",
    "            basic_by_nights = int(basic_policy[1]) * n_nights / 100\n",
    "            basic_percent = int(basic_policy[1])\n",
    "\n",
    "        data.loc[i, \"no_show_days\"] = no_show_days\n",
    "        data.loc[i, \"no_show_percentage\"] = no_show_percent\n",
    "\n",
    "        days = int(worse_policy[0])\n",
    "        basic_days = int(basic_policy[0])\n",
    "        data.loc[i, \"basic_charge_percentage\"] = basic_percent\n",
    "        data.loc[i, \"basic_charge_by_nights\"] = basic_by_nights\n",
    "        data.loc[i, \"basic_charge_days\"] = basic_days\n",
    "        # data.loc[i, \"basic_charge_days_times_nights\"] = basic_days * basic_by_nights\n",
    "        # data.loc[i, \"basic_charge_days_times_percentage\"] = basic_days * basic_percent\n",
    "        data.loc[i, \"charge_percentage\"] = percent\n",
    "        data.loc[i, \"charge_by_nights\"] = nights\n",
    "        data.loc[i, \"charge_days\"] = days\n",
    "        # data.loc[i, \"charge_days_times_nights\"] = days * nights\n",
    "        # data.loc[i, \"charge_days_times_percentage\"] = days * percent\n",
    "\n",
    "    # print('unknowns', counter)\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_data(full_data, test=False, cols=None):\n",
    "    # choose only numerical, dates, boolean and label:\n",
    "    if test:\n",
    "        data = full_data.loc[:, NUMERICAL_COLS + DATETIME_COLS + SHOULD_BE_BOOLEAN_COLS + BOOLEAN_COLS +\n",
    "                                CATEGORICAL_COLS + [\"cancellation_policy_code\"]]\n",
    "    else:\n",
    "        data = full_data.loc[:, NUMERICAL_COLS + DATETIME_COLS + SHOULD_BE_BOOLEAN_COLS + BOOLEAN_COLS +\n",
    "                                CATEGORICAL_COLS + [\"cancellation_policy_code\"] + [LABEL_COL]]\n",
    "\n",
    "    # Should we drop duplicates again? if so, we need to make sure we keep a unique identifier (either id or datetime?)\n",
    "    # data = data.drop_duplicates()\n",
    "\n",
    "    # todo one of the two?\n",
    "    #data.loc[:, LABEL_COL] = (pd.to_datetime(data[\"checkin_date\"]) -\n",
    "    #                          pd.to_datetime(data[LABEL_COL])).dt.days.between(2, 9).astype('int')\n",
    "\n",
    "    if not test:\n",
    "        data.loc[:, LABEL_COL] = data.loc[:, LABEL_COL].notnull()\n",
    "\n",
    "    # fill in 0 instead of None's\n",
    "    for col_name in SHOULD_BE_BOOLEAN_COLS:\n",
    "        data.loc[:, col_name] = data.loc[:, col_name].fillna(0)\n",
    "        data.loc[:, col_name] = data.loc[:, col_name] == 1\n",
    "\n",
    "    # handle datetime cols (convert to timestamps)\n",
    "    for col_name in DATETIME_COLS:\n",
    "        as_datetime = data[col_name]\n",
    "        data.loc[:, col_name + \"_year\"] = as_datetime.dt.year\n",
    "        data.loc[:, col_name + \"_month\"] = as_datetime.dt.month\n",
    "        data.loc[:, col_name + \"_day\"] = as_datetime.dt.day\n",
    "        data.loc[:, col_name + \"_day_in_week\"] = as_datetime.dt.day_of_week\n",
    "    \n",
    "    data.loc[:, \"n_nights\"] = (data[\"checkout_date\"] - data[\"checkin_date\"]).dt.days\n",
    "    data.loc[:, \"n_days_from_booking_to_checkin\"] = (data[\"checkin_date\"] - data[\"booking_datetime\"]).dt.days\n",
    "\n",
    "    data = data.drop(DATETIME_COLS, axis=1)\n",
    "    \n",
    "    data = parse_all_cancellation_policy(data)\n",
    "    data = data.drop(['cancellation_policy_code'], axis=1)\n",
    "\n",
    "    # replace categorical features with their dummies\n",
    "    data = pd.get_dummies(data, columns=CATEGORICAL_COLS, drop_first=True)\n",
    "    if cols is not None:\n",
    "        missing_cols = set(cols) - set(data.columns)\n",
    "        for col in missing_cols:\n",
    "            data[col] = 0\n",
    "        data = data[cols] \n",
    "    \n",
    "    if not test:\n",
    "        data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "50683cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename: str, test=False, cols=None):\n",
    "    \"\"\"\n",
    "    Load Agoda booking cancellation dataset\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        Path to house prices dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Design matrix and response vector in either of the following formats:\n",
    "    1) Single dataframe with last column representing the response\n",
    "    2) Tuple of pandas.DataFrame and Series\n",
    "    3) Tuple of ndarray of shape (n_samples, n_features) and ndarray of shape (n_samples,)\n",
    "    \"\"\"\n",
    "    full_data = pd.read_csv(filename, parse_dates=DATETIME_COLS)\n",
    "    if test:\n",
    "        return parse_data(full_data, test, cols)\n",
    "    full_data = full_data.drop_duplicates()\n",
    "    parsed_data = parse_data(full_data)\n",
    "    features, labels = parsed_data.loc[:, parsed_data.columns != LABEL_COL], parsed_data[LABEL_COL]\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9eb6d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle exporting data\n",
    "\n",
    "def evaluate_and_export(estimator: AgodaCancellationEstimator, X: np.ndarray, filename: str, y_true=None):\n",
    "    \"\"\"\n",
    "    Export to specified file the prediction results of given estimator on given testset.\n",
    "\n",
    "    File saved is in csv format with a single column named 'predicted_values' and n_samples rows containing\n",
    "    predicted values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: BaseEstimator or any object implementing predict() method as in BaseEstimator (for example sklearn)\n",
    "        Fitted estimator to use for prediction\n",
    "\n",
    "    X: ndarray of shape (n_samples, n_features)\n",
    "        Test design matrix to predict its responses\n",
    "\n",
    "    filename:\n",
    "        path to store file at\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = pd.DataFrame(estimator.predict(X))\n",
    "    if y_true is not None:\n",
    "        conf_matrix = confusion_matrix(y_true, predictions)\n",
    "        tn, fp, fn, tp = conf_matrix.ravel()\n",
    "        print(\"Accuracy\", accuracy_score(y_true, predictions))\n",
    "        print(f\"True negative: {tn}, False positive: {fp}, False Negative: {fn}, True Positive: {tp}\")\n",
    "        return # we want to write to the file only if it is the weekly test\n",
    "\n",
    "    pd.DataFrame(predictions.astype(int).to_numpy(), columns=[\"predicted_values\"]).to_csv(filename, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbeb2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing training data...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "    # Load data\n",
    "    print(\"Loading and preprocessing training data...\")\n",
    "    df, cancellation_labels = load_data(\"datasets/agoda_cancellation_train.csv\")\n",
    "\n",
    "    train_X, test_X, train_y, test_y = model_selection.train_test_split(df, cancellation_labels, random_state=0)\n",
    "\n",
    "    # # Fit model over data\n",
    "    print(\"Fitting the model over the preprocesed training data...\")\n",
    "    estimator = AgodaCancellationEstimator()\n",
    "    estimator.fit(train_X, train_y)\n",
    "    \n",
    "    print(\"Loading and preprocessing the weekly test data...\")\n",
    "    parsed_data = load_data(WEEKLY_TEST_SET, test=True, cols=train_X.columns)\n",
    "    parsed_data = parsed_data.reindex(columns=parsed_data.columns, fill_value=0)\n",
    "    \n",
    "    # # # Store model predictions over test set\n",
    "    print(\"Evaluating...\")\n",
    "    evaluate_and_export(estimator, test_X, \"{}_{}_{}.csv\".format(*STUDENTS), test_y)\n",
    "    \n",
    "    print(\"Predicting and exporting over the weekly test data...\")\n",
    "    evaluate_and_export(estimator, parsed_data, \"{}_{}_{}.csv\".format(*STUDENTS))\n",
    "    \n",
    "    print(\"Done :)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b986a6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a29c6e8d4ec1f1f9dc241f87ab0a94b82fc3375eb3b0393c457d990823ac03ba"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
